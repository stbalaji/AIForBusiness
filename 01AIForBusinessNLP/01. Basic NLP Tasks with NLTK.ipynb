{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# AI for Business by AIChampionsHub\n",
        "## Module : Natural Language Processing\n",
        "## Lesson : 01 - NLP Basics"
      ],
      "metadata": {
        "id": "2HJjRqv6zNhk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "NLP Course we try to balance between Theory, Practical and Business Application.\n",
        "This and other NLP Notebooks introduce key NLP Concepts.\n",
        "- NTLK : A popular library that helps balance between Theory and Practice.\n",
        "https://thinkinfi.com/how-to-download-nltk-corpus-manually/"
      ],
      "metadata": {
        "id": "g31LVuyKzWAk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "iRiFEDAnCbO3"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "7zlPfR7my6q1",
        "outputId": "dbed0de5-8dea-496c-969e-40f953b7bbe4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading collection 'tests'\n",
            "[nltk_data]    | \n",
            "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package averaged_perceptron_tagger is already up-\n",
            "[nltk_data]    |       to-date!\n",
            "[nltk_data]    | Downloading package porter_test to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping stemmers/porter_test.zip.\n",
            "[nltk_data]    | Downloading package twitter_samples to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package twitter_samples is already up-to-date!\n",
            "[nltk_data]    | Downloading package wmt15_eval to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping models/wmt15_eval.zip.\n",
            "[nltk_data]    | Downloading package subjectivity to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/subjectivity.zip.\n",
            "[nltk_data]    | Downloading package framenet_v17 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/framenet_v17.zip.\n",
            "[nltk_data]    | Downloading package product_reviews_1 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/product_reviews_1.zip.\n",
            "[nltk_data]    | Downloading package product_reviews_2 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/product_reviews_2.zip.\n",
            "[nltk_data]    | Downloading package vader_lexicon to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    | Downloading package crubadan to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/crubadan.zip.\n",
            "[nltk_data]    | Downloading package mte_teip5 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/mte_teip5.zip.\n",
            "[nltk_data]    | Downloading package sentence_polarity to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/sentence_polarity.zip.\n",
            "[nltk_data]    | Downloading package universal_treebanks_v20 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    | Downloading package panlex_swadesh to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    | Downloading package nonbreaking_prefixes to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/nonbreaking_prefixes.zip.\n",
            "[nltk_data]    | Downloading package perluniprops to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping misc/perluniprops.zip.\n",
            "[nltk_data]    | Downloading package pros_cons to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/pros_cons.zip.\n",
            "[nltk_data]    | Downloading package opinion_lexicon to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/opinion_lexicon.zip.\n",
            "[nltk_data]    | Downloading package comparative_sentences to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/comparative_sentences.zip.\n",
            "[nltk_data]    | \n",
            "[nltk_data]  Done downloading collection tests\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "nltk.download(\"tests\")  ## popular, tests, book Optional Step"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('punkt')\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize"
      ],
      "metadata": {
        "id": "sby4Wii2D-nD",
        "outputId": "37318b78-b040-42ea-96f7-91a22afbddad",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "InxhRaTZy6q6"
      },
      "outputs": [],
      "source": [
        "sents()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D_lK88Yky6q7",
        "outputId": "652b06f8-595b-462c-bddc-b0b5a5074af3"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['Call', 'me', 'Ishmael', '.']"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "sent1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "anxevxS5y6q9",
        "outputId": "592ea359-e803-4bde-99e2-bf646e0c2185"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "12408"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DIJBr9Cpy6q9",
        "outputId": "47fccbd8-a39d-4d55-b093-f5a578bca5f3"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['Pierre', 'Vinken', ',', '61', 'years', 'old', 'will', 'join', 'the', 'board']"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "vocab1 = list(dist.keys())\n",
        "vocab1[:10]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XGofk4yUy6q-",
        "outputId": "bd0fe7d0-9c30-4246-d1ce-80d2286306b4"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "2"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "dist['Vinken']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6uaCQLw9y6q-",
        "outputId": "e858aacb-ec3d-48fe-93c1-a3c6f4aaf9df"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['billion',\n",
              " 'company',\n",
              " 'president',\n",
              " 'because',\n",
              " 'market',\n",
              " 'million',\n",
              " 'shares',\n",
              " 'trading',\n",
              " 'program']"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "freqwords = [w for w in vocab1 if len(w) > 5 and dist[w] > 100]\n",
        "freqwords"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nLkfvhoVy6q_",
        "outputId": "1e87d638-ffad-4871-8a0c-13ad3134fcfb"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['list', 'listed', 'lists', 'listing', 'listings']"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hwh6eZt-y6q_",
        "outputId": "886c94ba-d3e3-441c-aa37-d13e0ef339f3"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['list', 'list', 'list', 'list', 'list']"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# different forms of the same \"word\"\n",
        "input1 = 'List listed lists listing listings'\n",
        "words1 = input1.lower().split(' ')\n",
        "words1\n",
        "porter = nltk.PorterStemmer()\n",
        "[porter.stem(t) for t in words1]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# BASICS"
      ],
      "metadata": {
        "id": "3P9d8mK1FE95"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 1 TOKENIZE\n",
        "Token is a sequence of characters in text that serves as a unit. Example tokens\n",
        "they could be words, emoticons, hashtags, links, or even individual characters.\n",
        "*   A basic way of breaking language into tokens is by splitting the text based on whitespace and punctuation.\n",
        "*   Then we can get words or numbers etc.\n",
        "\n"
      ],
      "metadata": {
        "id": "qY93rxRND7ox"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text1 = \"This is the first sentence. A gallon of milk in the U.S. costs $2.99. Is this the third sentence? Yes, it is! More costs \"\n",
        "text = text1"
      ],
      "metadata": {
        "id": "SsQEk67qFGtd"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentences = nltk.sent_tokenize(text)\n",
        "print(len(sentences))\n",
        "print(\"ORIGINAL TEXT : \", text)\n",
        "print(\"SENTENCE TOKENISER OUTPUT: \", sentences)"
      ],
      "metadata": {
        "id": "x2SDoonPFkK8",
        "outputId": "a34117f7-f644-456e-c2ed-734637ff93b3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "5\n",
            "ORIGINAL TEXT :  This is the first sentence. A gallon of milk in the U.S. costs $2.99. Is this the third sentence? Yes, it is! More costs \n",
            "SENTENCE TOKENISER OUTPUT:  ['This is the first sentence.', 'A gallon of milk in the U.S. costs $2.99.', 'Is this the third sentence?', 'Yes, it is!', 'More costs']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "words = nltk.word_tokenize(text)\n",
        "print(len(words))\n",
        "print(\"ORIGINAL TEXT : \", text)\n",
        "print(\"WORD TOKENISER OUTPUT: \", words)"
      ],
      "metadata": {
        "id": "vG4GoJ-pFqsd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "words_unique = set(words)\n",
        "list(set(words))[:10]"
      ],
      "metadata": {
        "id": "hr9WysyxKFUm",
        "outputId": "9ccc8f82-441e-44be-bcc0-851e4664fbb0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['third', ',', 'in', 'Is', 'This', 'this', 'A', 'the', 'U.S.', '!']"
            ]
          },
          "metadata": {},
          "execution_count": 58
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Find unique set of words etc."
      ],
      "metadata": {
        "id": "rDOKiUFWJ68g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Method 1 #  set () method: Used to convert any of the iterable to sequence of iterable elements with distinct elements\n",
        "from collections import Counter\n",
        "Counter(words)"
      ],
      "metadata": {
        "id": "u_gpJjwMG-hd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Method 2 : Frequency of words\n",
        "from nltk.probability import FreqDist\n",
        "dist = FreqDist(words)\n",
        "len(dist)\n",
        "dist"
      ],
      "metadata": {
        "id": "BwDclJVJJrsM",
        "outputId": "ec2589fb-d545-4fa7-a765-00e37cb83aa8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "FreqDist({'the': 3, 'is': 2, 'sentence': 2, '.': 2, 'costs': 2, 'This': 1, 'first': 1, 'A': 1, 'gallon': 1, 'of': 1, ...})"
            ]
          },
          "metadata": {},
          "execution_count": 57
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Give me all most frequently occuring words in a document or corpus with certain counts or thresholds\n",
        "# Try len > 3 or 4 etc...\n",
        "freqwords = set([w for w in words if len(w) > 1 and dist[w] > 1])\n",
        "freqwords"
      ],
      "metadata": {
        "id": "judq0VCAKVy6",
        "outputId": "94b9f42c-22f3-483f-d190-9e80a575c316",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'costs', 'is', 'sentence', 'the'}"
            ]
          },
          "metadata": {},
          "execution_count": 65
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2 Stemming"
      ],
      "metadata": {
        "id": "PTQca1yNJlpr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.probability import FreqDist"
      ],
      "metadata": {
        "id": "YE_BMGhqJnrt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## SENTIMENT ANALYSIS"
      ],
      "metadata": {
        "id": "rCRHLSuBE3ni"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.test import *"
      ],
      "metadata": {
        "id": "Lax788PrCkN5"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('twitter_samples')\n",
        "from nltk.corpus import twitter_samples"
      ],
      "metadata": {
        "id": "ii14Vaj2ENjY",
        "outputId": "cb3a8895-30e8-4c90-e357-028843ad2756",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package twitter_samples to /root/nltk_data...\n",
            "[nltk_data]   Package twitter_samples is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lX8GhR7zy6rA"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}