{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# AI for Business by AIChampionsHub\n",
        "## Module : Natural Language Processing\n",
        "## Lesson : 01 - NLP Basics"
      ],
      "metadata": {
        "id": "2HJjRqv6zNhk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "NLP Course we try to balance between Theory, Practical and Business Application.\n",
        "This and other NLP Notebooks introduce key NLP Concepts.\n",
        "- NTLK : A popular library that helps balance between Theory and Practice.\n",
        "https://thinkinfi.com/how-to-download-nltk-corpus-manually/"
      ],
      "metadata": {
        "id": "g31LVuyKzWAk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "iRiFEDAnCbO3"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7zlPfR7my6q1",
        "outputId": "dbed0de5-8dea-496c-969e-40f953b7bbe4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading collection 'tests'\n",
            "[nltk_data]    | \n",
            "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package averaged_perceptron_tagger is already up-\n",
            "[nltk_data]    |       to-date!\n",
            "[nltk_data]    | Downloading package porter_test to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping stemmers/porter_test.zip.\n",
            "[nltk_data]    | Downloading package twitter_samples to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package twitter_samples is already up-to-date!\n",
            "[nltk_data]    | Downloading package wmt15_eval to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping models/wmt15_eval.zip.\n",
            "[nltk_data]    | Downloading package subjectivity to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/subjectivity.zip.\n",
            "[nltk_data]    | Downloading package framenet_v17 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/framenet_v17.zip.\n",
            "[nltk_data]    | Downloading package product_reviews_1 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/product_reviews_1.zip.\n",
            "[nltk_data]    | Downloading package product_reviews_2 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/product_reviews_2.zip.\n",
            "[nltk_data]    | Downloading package vader_lexicon to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    | Downloading package crubadan to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/crubadan.zip.\n",
            "[nltk_data]    | Downloading package mte_teip5 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/mte_teip5.zip.\n",
            "[nltk_data]    | Downloading package sentence_polarity to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/sentence_polarity.zip.\n",
            "[nltk_data]    | Downloading package universal_treebanks_v20 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    | Downloading package panlex_swadesh to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    | Downloading package nonbreaking_prefixes to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/nonbreaking_prefixes.zip.\n",
            "[nltk_data]    | Downloading package perluniprops to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping misc/perluniprops.zip.\n",
            "[nltk_data]    | Downloading package pros_cons to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/pros_cons.zip.\n",
            "[nltk_data]    | Downloading package opinion_lexicon to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/opinion_lexicon.zip.\n",
            "[nltk_data]    | Downloading package comparative_sentences to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/comparative_sentences.zip.\n",
            "[nltk_data]    | \n",
            "[nltk_data]  Done downloading collection tests\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "nltk.download(\"tests\")  ## popular, tests, book Optional Step"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('punkt')\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sby4Wii2D-nD",
        "outputId": "37318b78-b040-42ea-96f7-91a22afbddad"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# BASICS"
      ],
      "metadata": {
        "id": "3P9d8mK1FE95"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 1 TOKENIZE\n",
        "Token is a sequence of characters in text that serves as a unit. Example tokens\n",
        "they could be words, emoticons, hashtags, links, or even individual characters.\n",
        "*   A basic way of breaking language into tokens is by splitting the text based on whitespace and punctuation.\n",
        "*   Then we can get words or numbers etc.\n",
        "\n"
      ],
      "metadata": {
        "id": "qY93rxRND7ox"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text1 = \"This is the first sentence. A gallon of milk in the U.S. costs $2.99. Is this the third sentence? Yes, it is! More costs \"\n",
        "text = text1"
      ],
      "metadata": {
        "id": "SsQEk67qFGtd"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentences = nltk.sent_tokenize(text)\n",
        "print(len(sentences))\n",
        "print(\"ORIGINAL TEXT : \", text)\n",
        "print(\"SENTENCE TOKENISER OUTPUT: \", sentences)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x2SDoonPFkK8",
        "outputId": "a34117f7-f644-456e-c2ed-734637ff93b3"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "5\n",
            "ORIGINAL TEXT :  This is the first sentence. A gallon of milk in the U.S. costs $2.99. Is this the third sentence? Yes, it is! More costs \n",
            "SENTENCE TOKENISER OUTPUT:  ['This is the first sentence.', 'A gallon of milk in the U.S. costs $2.99.', 'Is this the third sentence?', 'Yes, it is!', 'More costs']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "words = nltk.word_tokenize(text)\n",
        "print(len(words))\n",
        "print(\"ORIGINAL TEXT : \", text)\n",
        "print(\"WORD TOKENISER OUTPUT: \", words)"
      ],
      "metadata": {
        "id": "vG4GoJ-pFqsd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "words_unique = set(words)\n",
        "list(set(words))[:10]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hr9WysyxKFUm",
        "outputId": "9ccc8f82-441e-44be-bcc0-851e4664fbb0"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['third', ',', 'in', 'Is', 'This', 'this', 'A', 'the', 'U.S.', '!']"
            ]
          },
          "metadata": {},
          "execution_count": 58
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Find unique set of words etc."
      ],
      "metadata": {
        "id": "rDOKiUFWJ68g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Method 1 #  set () method: Used to convert any of the iterable to sequence of iterable elements with distinct elements\n",
        "from collections import Counter\n",
        "Counter(words)"
      ],
      "metadata": {
        "id": "u_gpJjwMG-hd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Method 2 : Frequency of words\n",
        "from nltk.probability import FreqDist\n",
        "dist = FreqDist(words)\n",
        "len(dist)\n",
        "dist"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BwDclJVJJrsM",
        "outputId": "ec2589fb-d545-4fa7-a765-00e37cb83aa8"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "FreqDist({'the': 3, 'is': 2, 'sentence': 2, '.': 2, 'costs': 2, 'This': 1, 'first': 1, 'A': 1, 'gallon': 1, 'of': 1, ...})"
            ]
          },
          "metadata": {},
          "execution_count": 57
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Give me all most frequently occuring words in a document or corpus with certain counts or thresholds\n",
        "# Try len > 3 or 4 etc...\n",
        "freqwords = set([w for w in words if len(w) > 1 and dist[w] > 1])\n",
        "freqwords"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "judq0VCAKVy6",
        "outputId": "94b9f42c-22f3-483f-d190-9e80a575c316"
      },
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'costs', 'is', 'sentence', 'the'}"
            ]
          },
          "metadata": {},
          "execution_count": 65
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2 Stemming"
      ],
      "metadata": {
        "id": "PTQca1yNJlpr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# different forms of the same \"word\"\n",
        "input1 = 'List listed lists listing listings'\n",
        "words1 = input1.lower().split(' ')\n",
        "words1\n",
        "\n",
        "porter = nltk.PorterStemmer()\n",
        "[porter.stem(t) for t in words1]"
      ],
      "metadata": {
        "id": "bBsBBj8PLLr5",
        "outputId": "9b242b36-9114-4103-e6aa-772b13d2590f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['list', 'list', 'list', 'list', 'list']"
            ]
          },
          "metadata": {},
          "execution_count": 67
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "set([porter.stem(t) for t in words1])"
      ],
      "metadata": {
        "id": "xGb96J6PLP1e",
        "outputId": "55baf0e0-190c-4b80-f0ab-1ea42e03a4c1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'list'}"
            ]
          },
          "metadata": {},
          "execution_count": 68
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# NLP EXAMPLE EXERCISE : 1 - SENTIMENT ANALYSIS"
      ],
      "metadata": {
        "id": "rCRHLSuBE3ni"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We use sample Tweets to perform the analysis.\n",
        "To use your own dataset, you can gather tweets from a specific time period, user, or hashtag by using the https://developer.twitter.com/en/docs.html"
      ],
      "metadata": {
        "id": "8q5GS-s0MjjK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.test import *"
      ],
      "metadata": {
        "id": "Lax788PrCkN5"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('twitter_samples')\n",
        "from nltk.corpus import twitter_samples"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ii14Vaj2ENjY",
        "outputId": "cb3a8895-30e8-4c90-e357-028843ad2756"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package twitter_samples to /root/nltk_data...\n",
            "[nltk_data]   Package twitter_samples is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {
        "id": "lX8GhR7zy6rA"
      },
      "outputs": [],
      "source": [
        "positive_tweets = twitter_samples.strings('positive_tweets.json')\n",
        "negative_tweets = twitter_samples.strings('negative_tweets.json')\n",
        "text = twitter_samples.strings('tweets.20150430-223406.json')   #  print all of the tweets within a dataset as strings"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## STEP 1 : Tokenzie the data\n",
        "Here we can use Punkt tokenizer. The punkt module is a pre-trained model that helps you tokenize words and sentences. Works on unsupervised data."
      ],
      "metadata": {
        "id": "mz77fKg2Lyy1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('punkt')"
      ],
      "metadata": {
        "id": "NmOKyl5fL7gR",
        "outputId": "918a4786-c4dc-47ec-cd75-fbe7524624c6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 73
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tweet_tokens = twitter_samples.tokenized('positive_tweets.json')"
      ],
      "metadata": {
        "id": "hyLsiVXXMBH_"
      },
      "execution_count": 74,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(tweet_tokens[0])"
      ],
      "metadata": {
        "id": "ijBVF3G4Mbcx",
        "outputId": "31c7cdf1-c5dd-4d1f-9678-851d29fe82f8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['#FollowFriday', '@France_Inte', '@PKuchly57', '@Milipol_Paris', 'for', 'being', 'top', 'engaged', 'members', 'in', 'my', 'community', 'this', 'week', ':)']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## STEP 1 : Normalize the data - Stemming and Lemmatization\n",
        "*    Like we saw before 'List, Lists, Listing' etc...may mean similar thinkgs but we don't want them to treat as different words.\n",
        "*    So normailization is used - group together words with the same meaning but different forms.\n",
        "*    Normalization in NLP is the process of converting a word to its canonical form.\n",
        "* Explore Two popular techniques : Stemming and Lemmatization\n",
        "\n"
      ],
      "metadata": {
        "id": "FXQ3GE6rNnYJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        ". Stemming, working with only simple verb forms, is a heuristic process that removes the ends of words."
      ],
      "metadata": {
        "id": "VF5Bkk5EOYXY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this tutorial you will use the process of lemmatization, which normalizes a word with the context of vocabulary and morphological analysis of words in text. The lemmatization algorithm analyzes the structure of the word and its context to convert it to a normalized form. Therefore, it comes at a cost of speed. A comparison of stemming and lemmatization ultimately comes down to a trade off between speed and accuracy."
      ],
      "metadata": {
        "id": "eA6-wjzkOc_X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('wordnet')  #wordnet is a lexical database for the English language that helps the script determine the base word.\n",
        "nltk.download('averaged_perceptron_tagger')  # to determine the context of a word in a sentence."
      ],
      "metadata": {
        "id": "xQwJxkVZOUJR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Before running a lemmatizer, you need to determine the context for each word in your text.\n",
        "# This is achieved by a tagging algorithm, which assesses the relative position of a word in a sentence.\n",
        "\n",
        "from nltk.tag import pos_tag\n",
        "from nltk.corpus import twitter_samples\n",
        "\n",
        "tweet_tokens = twitter_samples.tokenized('positive_tweets.json')\n",
        "print(pos_tag(tweet_tokens[0]))"
      ],
      "metadata": {
        "id": "r0uA2KvcO3qW",
        "outputId": "797fbf48-841d-49c2-9660-690bc0355b62",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('#FollowFriday', 'JJ'), ('@France_Inte', 'NNP'), ('@PKuchly57', 'NNP'), ('@Milipol_Paris', 'NNP'), ('for', 'IN'), ('being', 'VBG'), ('top', 'JJ'), ('engaged', 'VBN'), ('members', 'NNS'), ('in', 'IN'), ('my', 'PRP$'), ('community', 'NN'), ('this', 'DT'), ('week', 'NN'), (':)', 'NN')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "From the list of tags, here is the list of the most common items and their meaning:\n",
        "\n",
        "1.   NNP: Noun, proper, singular\n",
        "2.   NN: Noun, common, singular or mass\n",
        "3.   IN: Preposition or conjunction, subordinating\n",
        "4.   VBG: Verb, gerund or present participle\n",
        "5.   VBN: Verb, past participle"
      ],
      "metadata": {
        "id": "pOyWvotkPJFA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To incorporate this into a function that normalizes a sentence, you should first generate the tags for each token in the text, and then lemmatize each word using the tag.  \n",
        "Example : verb being changes to its root form, be, and the noun members changes to member"
      ],
      "metadata": {
        "id": "ALk7DqHZPel_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tag import pos_tag\n",
        "from nltk.stem.wordnet import WordNetLemmatizer\n",
        "\n",
        "def lemmatize_sentence(tokens):\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    lemmatized_sentence = []\n",
        "    for word, tag in pos_tag(tokens):\n",
        "        if tag.startswith('NN'):\n",
        "            pos = 'n'\n",
        "        elif tag.startswith('VB'):\n",
        "            pos = 'v'\n",
        "        else:\n",
        "            pos = 'a'\n",
        "        lemmatized_sentence.append(lemmatizer.lemmatize(word, pos))\n",
        "    return lemmatized_sentence"
      ],
      "metadata": {
        "id": "GfL2dwGhPAaL"
      },
      "execution_count": 80,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(lemmatize_sentence(tweet_tokens[0]))"
      ],
      "metadata": {
        "id": "vYYjDMpCPmcX",
        "outputId": "eeb1c8e7-af7f-4502-81a0-c143f10a79aa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['#FollowFriday', '@France_Inte', '@PKuchly57', '@Milipol_Paris', 'for', 'be', 'top', 'engage', 'member', 'in', 'my', 'community', 'this', 'week', ':)']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 4 — Removing Noise from the Data\n",
        "\n",
        "*   Noise is any part of the text that does not add meaning or information to data.\n",
        "*   Stop words : Example stop words are “is”, “the”, and “a”. They are generally not relevant when processing language, unless a specific use case warrants their inclusion.\n",
        "*   We use Regular Expressions and leverage a predefined or used function.\n",
        "Link for more details : https://docs.python.org/3.6/howto/regex.html\n",
        "\n",
        "*   Examples\n",
        "**  All hyperlinks in Twitter are converted to the URL shortener t.co. and therefore don't add much value in analysis.\n",
        "**  Replies:  Twitter handles in certain replies. These Twitter usernames are preceded by a @ symbol. Even these don't help on value from.\n",
        "**  Punctuation and special characters - While these often provide context to textual data, this context is often difficult to process. For simplicity, you will remove all punctuation and special characters from tweets.\n",
        "\n"
      ],
      "metadata": {
        "id": "wc9pnO49P6Qz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# LIST OF ALL STOPWORDS IN ENGLISH\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "stop_words = stopwords.words('english')\n",
        "# stop_words[:25]"
      ],
      "metadata": {
        "id": "SiZ19BTZRYrO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Code to remove noise, removes noise and incorporates the normalization and lemmatization.\n",
        "# Used sample from another site.\n",
        "import re, string\n",
        "\n",
        "def fn_Remove_Noise(tweet_tokens, stop_words = ()):\n",
        "\n",
        "    cleaned_tokens = []\n",
        "\n",
        "    for token, tag in pos_tag(tweet_tokens):\n",
        "        token = re.sub('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+#]|[!*\\(\\),]|'\\\n",
        "                       '(?:%[0-9a-fA-F][0-9a-fA-F]))+','', token)\n",
        "        token = re.sub(\"(@[A-Za-z0-9_]+)\",\"\", token)\n",
        "\n",
        "        if tag.startswith(\"NN\"):\n",
        "            pos = 'n'\n",
        "        elif tag.startswith('VB'):\n",
        "            pos = 'v'\n",
        "        else:\n",
        "            pos = 'a'\n",
        "\n",
        "        lemmatizer = WordNetLemmatizer()\n",
        "        token = lemmatizer.lemmatize(token, pos)\n",
        "\n",
        "        if len(token) > 0 and token not in string.punctuation and token.lower() not in stop_words:\n",
        "            cleaned_tokens.append(token.lower())\n",
        "    return cleaned_tokens"
      ],
      "metadata": {
        "id": "JWX-Jt3wQc20"
      },
      "execution_count": 82,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(fn_Remove_Noise(tweet_tokens[0], stop_words))fn_Remove_Noise"
      ],
      "metadata": {
        "id": "-ezyuE0RRh-H"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}