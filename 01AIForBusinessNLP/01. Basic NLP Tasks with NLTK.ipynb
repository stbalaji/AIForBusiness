{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# AI for Business by AIChampionsHub\n",
        "## Module : Natural Language Processing\n",
        "## Lesson : 01 - NLP Basics or Warm-up"
      ],
      "metadata": {
        "id": "2HJjRqv6zNhk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "NLP Course we try to balance between Theory, Practical and Business Application.\n",
        "This and other NLP Notebooks introduce key NLP Concepts.\n",
        "- NTLK : A popular library that helps balance between Theory and Practice.\n",
        "https://thinkinfi.com/how-to-download-nltk-corpus-manually/"
      ],
      "metadata": {
        "id": "g31LVuyKzWAk"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7zlPfR7my6q1",
        "outputId": "708c80fc-a772-474e-f290-05faec0ca2a9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading collection 'tests'\n",
            "[nltk_data]    | \n",
            "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data]    | Downloading package porter_test to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping stemmers/porter_test.zip.\n",
            "[nltk_data]    | Downloading package twitter_samples to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/twitter_samples.zip.\n",
            "[nltk_data]    | Downloading package wmt15_eval to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping models/wmt15_eval.zip.\n",
            "[nltk_data]    | Downloading package subjectivity to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/subjectivity.zip.\n",
            "[nltk_data]    | Downloading package framenet_v17 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/framenet_v17.zip.\n",
            "[nltk_data]    | Downloading package product_reviews_1 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/product_reviews_1.zip.\n",
            "[nltk_data]    | Downloading package product_reviews_2 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/product_reviews_2.zip.\n",
            "[nltk_data]    | Downloading package vader_lexicon to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    | Downloading package crubadan to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/crubadan.zip.\n",
            "[nltk_data]    | Downloading package mte_teip5 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/mte_teip5.zip.\n",
            "[nltk_data]    | Downloading package sentence_polarity to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/sentence_polarity.zip.\n",
            "[nltk_data]    | Downloading package universal_treebanks_v20 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    | Downloading package panlex_swadesh to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    | Downloading package nonbreaking_prefixes to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/nonbreaking_prefixes.zip.\n",
            "[nltk_data]    | Downloading package perluniprops to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping misc/perluniprops.zip.\n",
            "[nltk_data]    | Downloading package pros_cons to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/pros_cons.zip.\n",
            "[nltk_data]    | Downloading package opinion_lexicon to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/opinion_lexicon.zip.\n",
            "[nltk_data]    | Downloading package comparative_sentences to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/comparative_sentences.zip.\n",
            "[nltk_data]    | \n",
            "[nltk_data]  Done downloading collection tests\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "nltk.download(\"tests\")  ## popular, tests, book Optional Step"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Section 1 : Basic units of Text processing"
      ],
      "metadata": {
        "id": "-wfirtTwWmkL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*   **Token**: Technical name for a sequence of characters that we want to treat as group.\n",
        "*   **Count of Tokens** : Number of occurences of these sequences\n",
        "*   **Vocabulary** of a text: Set of tokens that it uses. Note: In set the duplicates are collapsed or removed - so only look at Unique combinations\n",
        "\n"
      ],
      "metadata": {
        "id": "iRiFEDAnCbO3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('punkt')\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sby4Wii2D-nD",
        "outputId": "41114998-7d7c-4ed0-f95e-2e988bf055f7"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sample = [\"to be or not to be\"]\n",
        "print(\"Count : \", len(sample))\n",
        "print(\"Unique elements : \", set(sample))"
      ],
      "metadata": {
        "id": "Twn0tARVXwt2",
        "outputId": "e27bc6a3-e73b-4f94-9975-62f3bf0b6298",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Count :  1\n",
            "Unique elements :  {'to be or not to be'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# NLP EXAMPLE EXERCISE : 1 - SENTIMENT ANALYSIS"
      ],
      "metadata": {
        "id": "rCRHLSuBE3ni"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We use sample Tweets to perform the analysis.\n",
        "To use your own dataset, you can gather tweets from a specific time period, user, or hashtag by using the https://developer.twitter.com/en/docs.html"
      ],
      "metadata": {
        "id": "8q5GS-s0MjjK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.test import *"
      ],
      "metadata": {
        "id": "Lax788PrCkN5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('twitter_samples')\n",
        "from nltk.corpus import twitter_samples"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ii14Vaj2ENjY",
        "outputId": "cb3a8895-30e8-4c90-e357-028843ad2756"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package twitter_samples to /root/nltk_data...\n",
            "[nltk_data]   Package twitter_samples is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lX8GhR7zy6rA"
      },
      "outputs": [],
      "source": [
        "positive_tweets = twitter_samples.strings('positive_tweets.json')\n",
        "negative_tweets = twitter_samples.strings('negative_tweets.json')\n",
        "text = twitter_samples.strings('tweets.20150430-223406.json')   #  print all of the tweets within a dataset as strings"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## STEP 1 : Tokenzie the data\n",
        "Here we can use Punkt tokenizer. The punkt module is a pre-trained model that helps you tokenize words and sentences. Works on unsupervised data."
      ],
      "metadata": {
        "id": "mz77fKg2Lyy1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tweet_tokens = twitter_samples.tokenized('positive_tweets.json')"
      ],
      "metadata": {
        "id": "hyLsiVXXMBH_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(tweet_tokens[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ijBVF3G4Mbcx",
        "outputId": "31c7cdf1-c5dd-4d1f-9678-851d29fe82f8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['#FollowFriday', '@France_Inte', '@PKuchly57', '@Milipol_Paris', 'for', 'being', 'top', 'engaged', 'members', 'in', 'my', 'community', 'this', 'week', ':)']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Cleaning Data : To be Updated\n",
        "Use of different text analysis methods may require different preprocessing. Some methods, like keyphrase search, work best when the text is “thoroughly cleaned”; i.e. almost reduced to a “bag of words” [28]. This means that, for instance, words are lemmatized, there is no punctuation, etc. However, some more recent techniques (like text embeddings [29]) can (and should) be trained on a “dirty” text, like Wikipedia [30] dumps or Common Crawl. Hence, it is necessary to distinguish between (at least) two levels of text\n",
        "cleaning: (A) “delicately cleaned” text (in what follows, called “Stage 1” cleaning), where only parts insignificant to the NLP analysis are removed, and (B) a “very strictly cleaned” text (called “Stage 2”cleaning).\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "9c8qHZRhbCOW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### All punctuation, numbers and other non-letter characters were removed, leaving only letters."
      ],
      "metadata": {
        "id": "nZhp4rGFbVoX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Adposition, adverb, conjunction, coordinating conjunction, determiner, interjection, numeral, particle,\n",
        "pronoun, punctuation, subordinating conjunction, symbol, end of line, space were removed. Parts of\n",
        "speech left after filtering were: verbs, nouns, auxiliaries and “other”. The “other” category is usually\n",
        "tagged for meaningless text, e.g. “asdfgh”. However, these were not deleted in case the algorithm\n",
        "detected something that was, in fact, important, e.g. domain-specific shortcuts and abbreviations like\n",
        "CNN, RNN, etc."
      ],
      "metadata": {
        "id": "HUiSWwJ4bc_L"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Words have been lemmatized."
      ],
      "metadata": {
        "id": "UGPY1P3WbkRL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## STEP 1 : Normalize the data - Stemming and Lemmatization\n",
        "*    Like we saw before 'List, Lists, Listing' etc...may mean similar thinkgs but we don't want them to treat as different words.\n",
        "*    So normailization is used - group together words with the same meaning but different forms.\n",
        "*    Normalization in NLP is the process of converting a word to its canonical form.\n",
        "* Explore Two popular techniques : Stemming and Lemmatization\n",
        "\n"
      ],
      "metadata": {
        "id": "FXQ3GE6rNnYJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        ". Stemming, working with only simple verb forms, is a heuristic process that removes the ends of words."
      ],
      "metadata": {
        "id": "VF5Bkk5EOYXY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this tutorial you will use the process of lemmatization, which normalizes a word with the context of vocabulary and morphological analysis of words in text. The lemmatization algorithm analyzes the structure of the word and its context to convert it to a normalized form. Therefore, it comes at a cost of speed. A comparison of stemming and lemmatization ultimately comes down to a trade off between speed and accuracy."
      ],
      "metadata": {
        "id": "eA6-wjzkOc_X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('wordnet')  #wordnet is a lexical database for the English language that helps the script determine the base word.\n",
        "nltk.download('averaged_perceptron_tagger')  # to determine the context of a word in a sentence."
      ],
      "metadata": {
        "id": "xQwJxkVZOUJR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Before running a lemmatizer, you need to determine the context for each word in your text.\n",
        "# This is achieved by a tagging algorithm, which assesses the relative position of a word in a sentence.\n",
        "\n",
        "from nltk.tag import pos_tag\n",
        "from nltk.corpus import twitter_samples\n",
        "\n",
        "tweet_tokens = twitter_samples.tokenized('positive_tweets.json')\n",
        "print(pos_tag(tweet_tokens[0]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r0uA2KvcO3qW",
        "outputId": "797fbf48-841d-49c2-9660-690bc0355b62"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('#FollowFriday', 'JJ'), ('@France_Inte', 'NNP'), ('@PKuchly57', 'NNP'), ('@Milipol_Paris', 'NNP'), ('for', 'IN'), ('being', 'VBG'), ('top', 'JJ'), ('engaged', 'VBN'), ('members', 'NNS'), ('in', 'IN'), ('my', 'PRP$'), ('community', 'NN'), ('this', 'DT'), ('week', 'NN'), (':)', 'NN')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "From the list of tags, here is the list of the most common items and their meaning:\n",
        "\n",
        "1.   NNP: Noun, proper, singular\n",
        "2.   NN: Noun, common, singular or mass\n",
        "3.   IN: Preposition or conjunction, subordinating\n",
        "4.   VBG: Verb, gerund or present participle\n",
        "5.   VBN: Verb, past participle"
      ],
      "metadata": {
        "id": "pOyWvotkPJFA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To incorporate this into a function that normalizes a sentence, you should first generate the tags for each token in the text, and then lemmatize each word using the tag.  \n",
        "Example : verb being changes to its root form, be, and the noun members changes to member"
      ],
      "metadata": {
        "id": "ALk7DqHZPel_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tag import pos_tag\n",
        "from nltk.stem.wordnet import WordNetLemmatizer\n",
        "\n",
        "def lemmatize_sentence(tokens):\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    lemmatized_sentence = []\n",
        "    for word, tag in pos_tag(tokens):\n",
        "        if tag.startswith('NN'):\n",
        "            pos = 'n'\n",
        "        elif tag.startswith('VB'):\n",
        "            pos = 'v'\n",
        "        else:\n",
        "            pos = 'a'\n",
        "        lemmatized_sentence.append(lemmatizer.lemmatize(word, pos))\n",
        "    return lemmatized_sentence"
      ],
      "metadata": {
        "id": "GfL2dwGhPAaL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(lemmatize_sentence(tweet_tokens[0]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vYYjDMpCPmcX",
        "outputId": "eeb1c8e7-af7f-4502-81a0-c143f10a79aa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['#FollowFriday', '@France_Inte', '@PKuchly57', '@Milipol_Paris', 'for', 'be', 'top', 'engage', 'member', 'in', 'my', 'community', 'this', 'week', ':)']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 4 — Removing Noise from the Data\n",
        "\n",
        "*   Noise is any part of the text that does not add meaning or information to data.\n",
        "*   Stop words : Example stop words are “is”, “the”, and “a”. They are generally not relevant when processing language, unless a specific use case warrants their inclusion.\n",
        "*   We use Regular Expressions and leverage a predefined or used function.\n",
        "Link for more details : https://docs.python.org/3.6/howto/regex.html\n",
        "\n",
        "*   Examples\n",
        "**  All hyperlinks in Twitter are converted to the URL shortener t.co. and therefore don't add much value in analysis.\n",
        "**  Replies:  Twitter handles in certain replies. These Twitter usernames are preceded by a @ symbol. Even these don't help on value from.\n",
        "**  Punctuation and special characters - While these often provide context to textual data, this context is often difficult to process. For simplicity, you will remove all punctuation and special characters from tweets.\n",
        "\n"
      ],
      "metadata": {
        "id": "wc9pnO49P6Qz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# LIST OF ALL STOPWORDS IN ENGLISH\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "stop_words = stopwords.words('english')\n",
        "stop_words[:25]"
      ],
      "metadata": {
        "id": "SiZ19BTZRYrO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Code to remove noise, removes noise and incorporates the normalization and lemmatization.\n",
        "# Used sample from another site.\n",
        "import re, string\n",
        "\n",
        "def fn_Remove_Noise(tweet_tokens, stop_words = ()):\n",
        "\n",
        "    cleaned_tokens = []\n",
        "\n",
        "    for token, tag in pos_tag(tweet_tokens):\n",
        "        token = re.sub('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+#]|[!*\\(\\),]|'\\\n",
        "                       '(?:%[0-9a-fA-F][0-9a-fA-F]))+','', token)\n",
        "        token = re.sub(\"(@[A-Za-z0-9_]+)\",\"\", token)\n",
        "\n",
        "        if tag.startswith(\"NN\"):\n",
        "            pos = 'n'\n",
        "        elif tag.startswith('VB'):\n",
        "            pos = 'v'\n",
        "        else:\n",
        "            pos = 'a'\n",
        "\n",
        "        lemmatizer = WordNetLemmatizer()\n",
        "        token = lemmatizer.lemmatize(token, pos)\n",
        "\n",
        "        if len(token) > 0 and token not in string.punctuation and token.lower() not in stop_words:\n",
        "            cleaned_tokens.append(token.lower())\n",
        "    return cleaned_tokens"
      ],
      "metadata": {
        "id": "JWX-Jt3wQc20"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#  function removes all @ mentions, stop words, and converts the words to lowercase\n",
        "print(fn_Remove_Noise(tweet_tokens[0], stop_words))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-ezyuE0RRh-H",
        "outputId": "b065fbe8-89d9-4cd4-8b6d-9da9fb9fced5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['#followfriday', 'top', 'engage', 'member', 'community', 'week', ':)']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Clean all the positive and negative tweets"
      ],
      "metadata": {
        "id": "PBrucWicSDaI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ositive_tweet_tokens = twitter_samples.tokenized('positive_tweets.json')\n",
        "negative_tweet_tokens = twitter_samples.tokenized('negative_tweets.json')\n",
        "\n",
        "positive_cleaned_tokens_list = []\n",
        "negative_cleaned_tokens_list = []\n",
        "\n",
        "positive_tweet_tokens = twitter_samples.tokenized('positive_tweets.json')\n",
        "negative_tweet_tokens = twitter_samples.tokenized('negative_tweets.json')\n",
        "\n",
        "for tokens in positive_tweet_tokens:\n",
        "    positive_cleaned_tokens_list.append(fn_Remove_Noise(tokens, stop_words))\n",
        "\n",
        "for tokens in negative_tweet_tokens:\n",
        "    negative_cleaned_tokens_list.append(fn_Remove_Noise(tokens, stop_words))"
      ],
      "metadata": {
        "id": "efmrDKnFSGvH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(positive_tweet_tokens[500])\n",
        "print(positive_cleaned_tokens_list[500])"
      ],
      "metadata": {
        "id": "sXE_5PNVSfEZ",
        "outputId": "f8a7ed54-d16f-4ca2-f113-cafde1e7da1e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Dang', 'that', 'is', 'some', 'rad', '@AbzuGame', '#fanart', '!', ':D', 'https://t.co/bI8k8tb9ht']\n",
            "['dang', 'rad', '#fanart', ':d']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 4 — Word Density, Frequency, Counts etc.\n",
        "\n",
        "*   Analyze of the frequency of words would be done on all positive tweets.\n",
        "*   Define a generator function (fn_get_all_words) that takes a list of tweets as an argument to provide a list of words in all of the tweet tokens joined."
      ],
      "metadata": {
        "id": "b1_3_tMoSpNv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def fn_get_all_words(cleaned_tokens_list):\n",
        "    for tokens in cleaned_tokens_list:\n",
        "        for token in tokens:\n",
        "            yield token\n",
        "\n",
        "all_pos_words = fn_get_all_words(positive_cleaned_tokens_list)"
      ],
      "metadata": {
        "id": "dxD4LNYhTEJq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "freq_dist_pos = FreqDist(all_pos_words)\n",
        "print(freq_dist_pos.most_common(10))     #  lists the words which occur most frequently in the data\n",
        "\n",
        "# Analyze the output - see emotion or gratitude words etc. like good or thanks etc."
      ],
      "metadata": {
        "id": "Vee5bqADTJ8l",
        "outputId": "d756d3a6-d29d-4ee9-d597-d319976b7653",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[(':)', 3691), (':-)', 701), (':d', 658), ('thanks', 388), ('follow', 357), ('love', 333), ('...', 290), ('good', 283), ('get', 263), ('thank', 253)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 5 — Model Building\n",
        "\n",
        "*   We can use Naive Bayes classifier in NLTK to perform the modeling.\n",
        "*   Model requires  list of words in a tweet, and Python dictionary with words as keys and True as values.\n",
        "*   Prepare data for the Model inputs by Converting Tokens to a Dictionary"
      ],
      "metadata": {
        "id": "1CDMaN8jTlZr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def fn_get_dictionary_tweets_for_model(cleaned_tokens_list):\n",
        "    for tweet_tokens in cleaned_tokens_list:\n",
        "        yield dict([token, True] for token in tweet_tokens)\n",
        "\n",
        "positive_tokens_for_model = fn_get_dictionary_tweets_for_model(positive_cleaned_tokens_list)\n",
        "negative_tokens_for_model = fn_get_dictionary_tweets_for_model(negative_cleaned_tokens_list)"
      ],
      "metadata": {
        "id": "iMtTEg9DTk3O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## TRAIN AND TEST SPLIT OF DATA"
      ],
      "metadata": {
        "id": "T0PRRKfOUQeG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "positive_dataset = [(tweet_dict, \"Positive\")  for tweet_dict in positive_tokens_for_model]\n",
        "negative_dataset = [(tweet_dict, \"Negative\")  for tweet_dict in negative_tokens_for_model]\n",
        "\n",
        "dataset = positive_dataset + negative_dataset\n",
        "random.shuffle(dataset)\n",
        "\n",
        "train_data = dataset[:7000]\n",
        "test_data = dataset[7000:]"
      ],
      "metadata": {
        "id": "QeppBTqPUP7G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Build and Test the Model"
      ],
      "metadata": {
        "id": "HqMP8YqlUaB8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk import classify\n",
        "from nltk import NaiveBayesClassifier\n",
        "classifier = NaiveBayesClassifier.train(train_data)\n",
        "\n",
        "print(\"Accuracy is:\", classify.accuracy(classifier, test_data))\n",
        "\n",
        "print(classifier.show_most_informative_features(10))"
      ],
      "metadata": {
        "id": "hmd1pRNPUeFV",
        "outputId": "d1cd347b-25be-4dbd-d5f7-6677ae508caa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy is: 0.9943333333333333\n",
            "Most Informative Features\n",
            "                      :( = True           Negati : Positi =   2061.2 : 1.0\n",
            "                      :) = True           Positi : Negati =   1665.7 : 1.0\n",
            "                follower = True           Positi : Negati =     36.9 : 1.0\n",
            "                     sad = True           Negati : Positi =     35.2 : 1.0\n",
            "                     bam = True           Positi : Negati =     24.1 : 1.0\n",
            "                 awesome = True           Positi : Negati =     21.4 : 1.0\n",
            "                    glad = True           Positi : Negati =     20.0 : 1.0\n",
            "                     x15 = True           Negati : Positi =     16.7 : 1.0\n",
            "               community = True           Positi : Negati =     15.3 : 1.0\n",
            "                 welcome = True           Positi : Negati =     15.0 : 1.0\n",
            "None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Analyze the above features.\n",
        "### What are the discriminating features for Sentiment?"
      ],
      "metadata": {
        "id": "yWixjOQrU53a"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Analysis of Output\n",
        "*   Accuracy is defined as the percentage of tweets in the testing dataset for which the model was correctly able to predict the sentiment. A 99.5% accuracy on the test set is pretty good."
      ],
      "metadata": {
        "id": "3mSmFDEiUkJb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_tweet1 = \"I ordered just from TerribleCo, they didn't deliver properly and screwed up. I never used the app again.\"\n",
        "test_tweet2 = \"Thank you for sending my baggage to CityX and flying me to CityY at the same time. Brilliant service. #thanksGenericAirline.\"\n",
        "test_tweet = test_tweet2"
      ],
      "metadata": {
        "id": "7oiB3ZRNVe6Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_tokens = fn_Remove_Noise(word_tokenize(test_tweet1))\n",
        "print(classifier.classify(dict([token, True] for token in test_tokens)))"
      ],
      "metadata": {
        "id": "OG-DuNlKVLgC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Named Entity Recognition or NER\n",
        "\n",
        "*   Named Entity Recognition–NER : NER is about  finding an answer to “the problem of locating and categorizing important nouns and proper nouns, in a text”\n",
        "*   Automatic methods should facilitate extraction of say named topics, issues, problems, and other “things” mentioned in texts (e.g. in\n",
        "articles).\n",
        "*   Library: the spaCy NER model (like **“en-core-web-lg**”) can be used to extract named entities. These entities have been linked by co-occurrence, and visualized as networks.\n",
        "*   SpaCy is simpler to use, and performed faster relative ot others like Transformers"
      ],
      "metadata": {
        "id": "ndN2dIy6b26v"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "npjHPsIjcgHZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "eYwjgp4-cevx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "M7KSxC9ocQI0"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "colab": {
      "provenance": [],
      "toc_visible": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}