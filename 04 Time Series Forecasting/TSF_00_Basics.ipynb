{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":2592705,"sourceType":"datasetVersion","datasetId":1342431},{"sourceId":3212237,"sourceType":"datasetVersion","datasetId":1449102}],"dockerImageVersionId":30096,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"This is part of a series of notebooks about practical time series methods:\n\n* [Part 0: the basics](https://www.kaggle.com/konradb/ts-0-the-basics) - **this notebook**\n* [Part 1a: smoothing methods](https://www.kaggle.com/konradb/ts-1a-smoothing-methods)\n* [Part 1b: Prophet](https://www.kaggle.com/konradb/ts-1b-prophet) \n* [Part 2: ARMA](https://www.kaggle.com/konradb/ts-2-arma-and-friends)\n* [Part 3: Time series for finance](https://www.kaggle.com/konradb/ts-3-time-series-for-finance) \n* [Part 4: Sales and demand forecasting](https://www.kaggle.com/konradb/ts-4-sales-and-demand-forecasting)\n* [Part 5: Automatic for the people](https://www.kaggle.com/code/konradb/ts-5-automatic-for-the-people) \n* [Part 6: Deep learning for TS - sequences](https://www.kaggle.com/konradb/ts-6-deep-learning-for-ts-sequences)\n* [Part 7: Survival analysis](https://www.kaggle.com/konradb/ts-7-survival-analysis)\n* [Part 8: Hierarchical time series](https://www.kaggle.com/code/konradb/ts-8-hierarchical-time-series)\n* [Part 9: Hybrid methods](https://www.kaggle.com/code/konradb/ts-9-hybrid-methods/)\n* [Part 10: Validation methods for time series](https://www.kaggle.com/code/konradb/ts-10-validation-methods-for-time-series/)\n* [Part 11: Transfer learning](https://www.kaggle.com/code/konradb/ts-11-deep-learning-for-ts-transfer-learning)\n\n\nThe series is accompanied by video presentations on the YouTube channel of [Abhishek](https://www.kaggle.com/abhishek):\n\n* [Talk 0](https://www.youtube.com/watch?v=cKzXOOtOXYY) - **based on this notebook**\n* [Talk 1](https://www.youtube.com/watch?v=kAI67Sz92-s) - combining the content from parts 1a and 1b\n* [Talk 2](https://www.youtube.com/watch?v=LjV5DE3KR-U) \n* [Talk 3](https://www.youtube.com/watch?v=74rDhJexmTg)\n* [Talk 4](https://www.youtube.com/watch?v=RdH8zd07u2E) \n* [Talk 5](https://www.youtube.com/watch?v=wBP8Pc4Wxzs)\n* [Talk 6](https://www.youtube.com/watch?v=81AEI0tj0Kk)\n* [Talk 7](https://www.youtube.com/watch?v=m-8I_hkmz9o)\n* [Talk 8](https://www.youtube.com/watch?v=7ZTarg4QYR4)\n* [Talk 9](https://www.youtube.com/watch?v=NYZzBvKcfp4)\n* [Talk 10](https://www.youtube.com/watch?v=47WeBiLV2Uo)\n* [Talk 11]()\n\n\n**If you think this notebook deserves an upvote, I'd love to have it. An upvote per view, its all I ask**\n(credit to [Dan Carlin](https://twitter.com/HardcoreHistory) for coining the phrase ;-) \n\n\n---------------------------------------\n\n\nThis notebook summarizes some elementary methods for time series analysis - sometimes you don't have the time / hardware / data to go for a Transformer, and vintage methods can be your friend. There is more content where that came from (based on a course I used to teach), so further installments will involve ARIMA, state space methods (and eventually sequence models - because #DeepLearnEverything ;-) \n\n* [Groundwork](#section-one)\n* [Patterns](#section-two)\n* [Dependence](#section-three)\n* [Stationarity](#section-four)\n\n\nWe start by importing the necessary libraries - most of them are familiar to anybody working with the data science, with the exception of statsmodels. It is a product of impressive work by Seabold and Perktold (http://conference.scipy.org/proceedings/scipy2010/pdfs/seabold.pdf) - two people who set out to bring statistical functionality in Python into the 21st century. If you are likely to use statistics in your work and you are a Pythonista, familiarizing yourself with this library is a very good idea: \n\nhttps://www.statsmodels.org/stable/index.html\n\nIn this module we are merely scratching the surface of statsmodel functionality, with seasonal decomposition as our primary tool.","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom random import gauss\nfrom pandas.plotting import autocorrelation_plot\nimport warnings\nimport itertools\nfrom random import random\n\nimport statsmodels.formula.api as smf\nimport statsmodels.api as sm\nfrom statsmodels.graphics.tsaplots import plot_acf, plot_pacf\nfrom statsmodels.tsa.ar_model import AR\nfrom statsmodels.tsa.seasonal import seasonal_decompose\nfrom statsmodels.tsa.stattools import adfuller\n\nimport matplotlib.pyplot as plt\nplt.style.use('fivethirtyeight') \n# import matplotlib as mpl\nimport seaborn as sns     \n","metadata":{"execution":{"iopub.status.busy":"2021-07-10T17:43:17.095884Z","iopub.execute_input":"2021-07-10T17:43:17.09634Z","iopub.status.idle":"2021-07-10T17:43:18.614573Z","shell.execute_reply.started":"2021-07-10T17:43:17.09625Z","shell.execute_reply":"2021-07-10T17:43:18.613857Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# general settings\nclass CFG:\n    data_folder = '../input/tsdata-1/'\n    img_dim1 = 20\n    img_dim2 = 10\n    \n    \n# adjust the parameters for displayed figures    \nplt.rcParams.update({'figure.figsize': (CFG.img_dim1,CFG.img_dim2)})    ","metadata":{"execution":{"iopub.status.busy":"2021-07-06T09:00:13.008177Z","iopub.execute_input":"2021-07-06T09:00:13.008706Z","iopub.status.idle":"2021-07-06T09:00:13.019288Z","shell.execute_reply.started":"2021-07-06T09:00:13.008658Z","shell.execute_reply":"2021-07-06T09:00:13.018143Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"section-one\"></a>\n# Groundwork\n\n\nTL;DR Time series is any sequence you record over time and applications are everywhere.\n\n\n\nMore formally, time series data is a sequence of data points (or observations) recorded at different time intervals - those intervals are frequently, but not always, regular (hourly, daily, weekly, monthly, quarterly etc): \n\n\\begin{equation}\n\\{X_t\\} \\quad t= 1,\\ldots,T \n\\end{equation}\n\nA strict formulation would be that a time series (discrete) realization of a (continuous) stochastic process generating the data and the underlying reason why we can infer from the former about the latter is the Kolmogorov extension theorem. The proper mathematical treatment of this theory is way beyond the scope of this notebook, so a mathematically inclinded reader is advised to look up those terms and then follow the references. ","metadata":{}},{"cell_type":"markdown","source":"Phenomena measured over time are everywhere, so a natural question is: what can we do with time series? Some of the more popular applications / reasons to bother are:\n\n* interpretation: we want to be able to make sense of diverse phenomena and capture the nature of the underlying dynamics\n* modelling: understanding inherent aspects of the time series data so that we can create meaningful and accurate forecasts.\n* **forecasting / prediction**: we want to know something about the future :-) \n* filtering / smoothing: we want to get a better understanding of the process based on partially / fully observed sample\n* simulation: in certain applications calculating e.g. high quantiles of a distribution is only possible with simulation, because there is not enough historical data","metadata":{}},{"cell_type":"markdown","source":"Some useful references include:\n\n    * Brockwell and Davies \"Time Series: Theory and Methods\"\n    * Shumway and Stoffer \"Time Series Analysis and Its Applications\"\n    * Durbin and Koopman \"Time Series Analysis by State Space Methods\"\n    * Just about anything written by **Rob Hyndman** https://robjhyndman.com/\n    * Ross Ihaka \"Time Series Analysis\": https://www.stat.auckland.ac.nz/~ihaka/726/notes.pdf","metadata":{}},{"cell_type":"markdown","source":"<a id=\"section-two\"></a>\n# Patterns","metadata":{}},{"cell_type":"markdown","source":"The first we can do to identify patterns in a time series is separate it into components with easily understandable characteristics:\n\n\\begin{equation}\nX_t = T_t + S_t + C_t + I_t \\quad \n\\end{equation}\n\nwhere: \n* $T_t$: the trend shows a general direction of the time series data over a long period of time. It represents a long-term progression of the series (secular variation)  \n\n* $S_t$: the seasonal component with fixed and known period. It is observed when there is a distinct repeated pattern observed between regular intervals due to seasonal factors: annual, monthly or weekly. Obvious examples include daily power consumption patterns or annual sales of seasonal goods.\n\n* $C_t$: (optional) cyclical component is a repetitive pattern which does not occur at fixed intervals - usually observed in an economic context like business cycles. \n\n* $I_t$: the irregular component (residuals ) consists of the fluctuations in the time series that are observed after removing trend and seasonal / cyclical variations. ","metadata":{}},{"cell_type":"markdown","source":"We may have different combinations of trends and seasonality. Depending on the nature of the trends and seasonality, a time series can be modeled as an additive or multiplicative time series. Each observation in the series can be expressed as either a sum or a product of the components.\n","metadata":{}},{"cell_type":"markdown","source":"It is worth pointing out that an alternative to using a multiplicative decomposition is to first transform the data until the variation in the series appears to be stable over time, then use an additive decomposition. When a log transformation has been used, this is equivalent to using a multiplicative decomposition because\n\\begin{equation}\nX_t = T_t * S_t * I_t\n\\end{equation}\n\nis equivalent to \n\n\\begin{equation}\nlog X_t = log T_t + log S_t + log I_t\n\\end{equation}\n\n\nA popular implementation for calculating the fundamental decomposition can be used via the statsmodels package:","metadata":{}},{"cell_type":"code","source":"help(seasonal_decompose)","metadata":{"execution":{"iopub.status.busy":"2021-07-06T09:00:13.020557Z","iopub.execute_input":"2021-07-06T09:00:13.020883Z","iopub.status.idle":"2021-07-06T09:00:13.033115Z","shell.execute_reply.started":"2021-07-06T09:00:13.020851Z","shell.execute_reply":"2021-07-06T09:00:13.031939Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Examples","metadata":{}},{"cell_type":"markdown","source":"### Passengers dataset\n\nEvery field of knowledge has **the** dataset that is used for teaching purposes: machine learning has Iris and CIFAR, differential equations - Canadian lynx data, and statistics has the airline passengers dataset between 1949 and 1960, first compiled by Box and Jenkins (you will be hearing those two names again in the next module) in 1976. We will use this dataset to demonstrate in practice what kind of information can be obtained using seasonal decomposition.\n\n<!-- Box, G. E. P., Jenkins, G. M. and Reinsel, G. C. (1976) Time Series Analysis, Forecasting and Control. Third Edition. Holden-Day. Series G. -->","metadata":{}},{"cell_type":"code","source":"series = pd.read_csv(CFG.data_folder + 'passengers.csv')\nseries['date'] = pd.to_datetime(series['date'])\nseries.set_index('date').plot()\n","metadata":{"execution":{"iopub.status.busy":"2021-07-06T09:00:13.034395Z","iopub.execute_input":"2021-07-06T09:00:13.034671Z","iopub.status.idle":"2021-07-06T09:00:13.283569Z","shell.execute_reply.started":"2021-07-06T09:00:13.034643Z","shell.execute_reply":"2021-07-06T09:00:13.28258Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's start with a basic additive decomposition:","metadata":{}},{"cell_type":"code","source":"# decomposition\ndecomposition = sm.tsa.seasonal_decompose(series[\"passengers\"],period =12) \nfigure = decomposition.plot()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-07-06T09:00:13.285102Z","iopub.execute_input":"2021-07-06T09:00:13.285416Z","iopub.status.idle":"2021-07-06T09:00:13.941068Z","shell.execute_reply.started":"2021-07-06T09:00:13.285384Z","shell.execute_reply":"2021-07-06T09:00:13.939869Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Trend and seasonality are behaving more or less in line with expectations, but the behavior of the residuals is clearly not consistent over time (average level of oscillations in the middle of the sample is very different than on either end). While there are many possible reasons, one quick explanation is the additive vs multiplicative relationship between the series components - which is something we can examine quickly:","metadata":{}},{"cell_type":"code","source":"decomposition = sm.tsa.seasonal_decompose(series[\"passengers\"],period =12, model = 'multiplicative') \nfigure = decomposition.plot()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-07-06T09:00:13.942328Z","iopub.execute_input":"2021-07-06T09:00:13.942628Z","iopub.status.idle":"2021-07-06T09:00:14.61644Z","shell.execute_reply.started":"2021-07-06T09:00:13.9426Z","shell.execute_reply":"2021-07-06T09:00:14.615356Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Not much of a qualitative change in trend and seasonality components, but the residuals looks much more stable around a constant level - such phenomenon does not of course imply stationarity by itself, but at least a clear signal in the opposite direction is not there anymore. ","metadata":{}},{"cell_type":"markdown","source":"### Changes in level of savings in the US\n\nLet's check how does seasonal decomposition work with some other popular datasets:","metadata":{}},{"cell_type":"code","source":"series = pd.read_csv(CFG.data_folder + 'savings_change.csv')\nseries['date'] = pd.to_datetime(series['date'])\nseries.set_index('date').plot()\n","metadata":{"execution":{"iopub.status.busy":"2021-07-06T09:00:15.567583Z","iopub.execute_input":"2021-07-06T09:00:15.56791Z","iopub.status.idle":"2021-07-06T09:00:15.955152Z","shell.execute_reply.started":"2021-07-06T09:00:15.567879Z","shell.execute_reply":"2021-07-06T09:00:15.954108Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# decomposition\ndecomposition = sm.tsa.seasonal_decompose(series[\"delta_val\"],period =12) \nfigure = decomposition.plot()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-07-06T09:00:15.956396Z","iopub.execute_input":"2021-07-06T09:00:15.95668Z","iopub.status.idle":"2021-07-06T09:00:16.633754Z","shell.execute_reply.started":"2021-07-06T09:00:15.956652Z","shell.execute_reply":"2021-07-06T09:00:16.632799Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Annual averages of the daily sunspot areas","metadata":{}},{"cell_type":"code","source":"\nseries = pd.read_csv(CFG.data_folder + 'sunspots.csv')\nseries['date'] = pd.to_datetime(series['date'])\nseries.set_index('date').plot()\n","metadata":{"execution":{"iopub.status.busy":"2021-07-06T09:00:16.635036Z","iopub.execute_input":"2021-07-06T09:00:16.635349Z","iopub.status.idle":"2021-07-06T09:00:17.12318Z","shell.execute_reply.started":"2021-07-06T09:00:16.635318Z","shell.execute_reply":"2021-07-06T09:00:17.122151Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"decomposition = sm.tsa.seasonal_decompose(series[\"avg_sunspot_area\"],period =12) \nfigure = decomposition.plot()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-07-06T09:00:17.124564Z","iopub.execute_input":"2021-07-06T09:00:17.124928Z","iopub.status.idle":"2021-07-06T09:00:17.782959Z","shell.execute_reply.started":"2021-07-06T09:00:17.124893Z","shell.execute_reply":"2021-07-06T09:00:17.781969Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"decomposition = sm.tsa.seasonal_decompose(series[\"avg_sunspot_area\"],period =12, model = 'multiplicative') \nfigure = decomposition.plot()\nplt.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"As we can see, changing the decomposition type to multiplicative alleviates the problem with residual behavior, but only to a certain degree - the pattern is still not consistent, with increase in the amplitude in the middle of the sample. This demonstrates that while seasonal decomposition is a fast tool, it has severe limitations when dealing with more sophisticated data generating processes.","metadata":{}},{"cell_type":"markdown","source":"<a id=\"section-three\"></a>\n# Dependence\n\nThe only way to succesful prediction is if past values of a series carry some information about the future behavior - in other words, if the present values are dependent on the past. A fast - and therefore very popular - manner of examining this dependence are the autocorrelation and partial autocorrelation functions, which are defined below:","metadata":{}},{"cell_type":"markdown","source":"Mean function of time series:\n\\begin{equation}\n\\mu_t = E [X_t]\n\\end{equation}\n\nAutocovariance function of a time series:\n\\begin{equation}\n\\gamma(s,t) = Cov(X_s, X_t) = E [X_s X_t] - E[X_s]E[X_t]\n\\end{equation}","metadata":{}},{"cell_type":"markdown","source":"which leads to the following definitions of ACF / PACF:","metadata":{}},{"cell_type":"markdown","source":"Autocorrelation:\n\\begin{equation}\n\\rho(u,t+u) = Cor(X_{u}, X_{t+u}) = \\frac{Cov(X_t, X_{t+u})}{Var(X_t) Var(X_{t+u})}\n\\end{equation}","metadata":{}},{"cell_type":"markdown","source":"Partial autocorrelation:\n    \\begin{equation}\n    \\phi(u) = Cor(X_t, X_{t+u}|X_{t+1}, \\ldots , X_{t+u-1})\n    \\end{equation}","metadata":{}},{"cell_type":"markdown","source":"An intuitive way to think about it is that ACF at lag $k$ measures a linear dependence between $X_t$ and $X_{t+k}$, while PACF captures the dependence between those values **correcting** for all the intermediate effects. ","metadata":{}},{"cell_type":"markdown","source":"<a id=\"section-four\"></a>\n# Stationarity","metadata":{}},{"cell_type":"code","source":"import os\nfrom IPython.display import Image\nImage(filename=\"../input/muh-images/gwozdzie.jpg\", width= 600, height=200)","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-07-06T09:12:37.740264Z","iopub.execute_input":"2021-07-06T09:12:37.740753Z","iopub.status.idle":"2021-07-06T09:12:37.766332Z","shell.execute_reply.started":"2021-07-06T09:12:37.740712Z","shell.execute_reply":"2021-07-06T09:12:37.765264Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"\nThe reason why bother with the concept in stationarity can be is best summarized with a bad joke I heard many years ago as a math student: say, a mathematician is given a piece of wood with two nails - one hammered all the way in, the other half there - and told to get remove them. How does he solve it?\n1. start with the nail that's completetely in - that's the more interesting case\n2. once done, hammer the other all the way - to reduce to an already solved case\n\nI warned you it was bad - but there is an analogy to how time series theory handles the problem of nonstationarity. \n\nTo define things more formally, a stationary time series is one whose unconditional joint probability distribution does not change when shifted in time. This implies that parameters such as mean and variance also do not change over time.\n\nSince stationarity is an assumption underlying many statistical procedures used in time series analysis, non-stationary data are often transformed to become stationary. A trend stationary process is not strictly stationary, but can easily be transformed into a stationary process by removing the underlying trend, which is solely a function of time; the same holds true for a stationary process with an added cyclical component. \n\nThe core idea is that it's much easier to model dynamic behavior over time if the statistical properties do not change: oscillations happen around the same level, the amplitude does not change too much etc (in other words, the probability distribution of $X_t$ is the same as the distribution of $X_{t+h}$)- such models are well understood. Algorithms are likely to yield better predictions if we apply them to stationary processes, because we do not need to worry about e.g. concept drift between our training and test sets. \n\n\nIf we are dealing with a process that does not adhere to those characteristics, we can either try and capture them directly or transform it in such a manner that it can be considered stationary. \n\n \nBelow we plot a few examples of stationary and non-stationary series, starting with the simplest non-trivial stationary series: Gaussian white noise.","metadata":{}},{"cell_type":"code","source":"xseries = pd.DataFrame(data = np.random.normal(0, 1, 10000), columns = ['noise'] )\nxseries.plot()\nprint()","metadata":{"execution":{"iopub.status.busy":"2021-07-10T20:56:43.342728Z","iopub.execute_input":"2021-07-10T20:56:43.343374Z","iopub.status.idle":"2021-07-10T20:56:43.420687Z","shell.execute_reply.started":"2021-07-10T20:56:43.343261Z","shell.execute_reply":"2021-07-10T20:56:43.41963Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_acf(xseries['noise'], lags = 25)\nprint()","metadata":{"execution":{"iopub.status.busy":"2021-07-06T09:12:44.96734Z","iopub.execute_input":"2021-07-06T09:12:44.967751Z","iopub.status.idle":"2021-07-06T09:12:45.232537Z","shell.execute_reply.started":"2021-07-06T09:12:44.967713Z","shell.execute_reply":"2021-07-06T09:12:45.231457Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_pacf(xseries['noise'], lags = 25)\nprint()","metadata":{"execution":{"iopub.status.busy":"2021-07-06T09:12:48.737631Z","iopub.execute_input":"2021-07-06T09:12:48.738003Z","iopub.status.idle":"2021-07-06T09:12:48.981513Z","shell.execute_reply.started":"2021-07-06T09:12:48.737972Z","shell.execute_reply":"2021-07-06T09:12:48.980567Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"For a slightly more interesting example (of non-stationary behavior), we can examine the passengers dataset:","metadata":{}},{"cell_type":"code","source":"# Non-stationary example\nseries = pd.read_csv(CFG.data_folder + 'passengers.csv')\nseries['date'] = pd.to_datetime(series['date'])\nseries.set_index('date').plot()","metadata":{"execution":{"iopub.status.busy":"2021-07-06T09:12:51.979226Z","iopub.execute_input":"2021-07-06T09:12:51.979574Z","iopub.status.idle":"2021-07-06T09:12:52.228577Z","shell.execute_reply.started":"2021-07-06T09:12:51.979543Z","shell.execute_reply":"2021-07-06T09:12:52.227387Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The stationarity of a series can be checked by examining the distribution of the series: we split the series into 2 contiguous parts and compute the summary statistics like the mean, variance and the autocorrelation. If the stats are quite different, then the series is not likely to be stationary.","metadata":{}},{"cell_type":"code","source":"series = pd.read_csv(CFG.data_folder + 'passengers.csv')\nseries['date'] = pd.to_datetime(series['date'])\n# series.set_index('date').plot()\n\nseries['passengers'].plot.hist(bins=25, alpha=0.5)\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Compute the summary statistics:","metadata":{}},{"cell_type":"code","source":"X = series.passengers.values\nsplit =  int(len(X) / 2)\nX1, X2 = X[0:split], X[split:]\nmean1, mean2 = X1.mean(), X2.mean()\nvar1, var2 = X1.var(), X2.var()\nprint('mean:')\nprint('chunk1: %.2f vs chunk2: %.2f' % (mean1, mean2))\nprint('variance:')\nprint('chunk1: %.2f vs chunk2: %.2f' % (var1, var2))\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The values are clearly very different across the two data subsets, which strongly suggests non-stationarity. However, visual inspection is not what one would could a rigorous criterion - so let's define things in a formal manner.","metadata":{}},{"cell_type":"markdown","source":"## (A little bit of) theory","metadata":{}},{"cell_type":"markdown","source":"Given a stochastic process $\\{X_t\\}$ and cdf $F_X$, a process is strictly stationary iff \n\n\\begin{equation}\nF_X(X_{t_1}, \\ldots, X_{t_n}) = F_X(X_{t_1 + \\tau}, \\ldots, X_{t_n + \\tau})\n\\end{equation}\n\nThe definition is very powerful (if we know a cdf of a distribution, we can infer everything) - however, it is not possible to verify in practice. For that reason, a less strict variant has been introduced: weak a.k.a. second order stationarity:\n\na process $\\{X_t\\}$ is weakly stationary if it satisfies the following conditions:\n\n* $\\mu_t = \\mu_{t + \\tau}$\n* $\\gamma(s,t) = \\gamma(s-t)$\n* $Var(X_t) < \\infty$","metadata":{}},{"cell_type":"markdown","source":"Implication:\n* constant mean\n* covariance only depends on distance in time between variables\n* autocorrelation:\n    \\begin{equation}\n    \\rho(u) = \\frac{\\gamma(u)}{\\gamma(0)}\n    \\end{equation}\n    \n    \nSo how can we turn a non-stationary series into a stationary one? Popular transformations include (but are not limited to!): \n* differencing the series \n* taking the log of the series\n* power transforms \n","metadata":{}},{"cell_type":"markdown","source":"### Differencing","metadata":{}},{"cell_type":"markdown","source":"* Lag operator of order $d$:\n\n\\begin{equation}\n\\nabla_d X_t = X_t - X_{t-d}\n\\end{equation}\n    \nDifferencing can help stabilize the mean of a time series by removing changes in the level of a time series, and so eliminating trend and seasonality. Differencing at lag 1 is best thought of as discreet counterpart to differentiation: first derivative of a linear function is flat, first derivative of a quadratic function is linear etc - so if we want to get rid of a trend behaving like polynomial of degree $n$, we need to apply the differencing operator $n$ times. \n\n\nIt is important to understand the difference between iterating diffencing operator $n$ times and differencing once at lag $n$ - which is best demonstrated in an example. ","metadata":{}},{"cell_type":"markdown","source":"### Tests for stationarity\n\nWhile inspecting plots before / after transformation can be useful to assess presence of trends or seasonalities (as we did above with the passengers dataset), in practice we need a more formal approach - like testing a hypothesis (introduction to statistical tests is beyond the scope of this notebook, so if you feel like you need a refresher, please consult other sources). The most popular tests dealing with stationarity are: \n\n* Augmented Dickey-Fuller (ADF))\n* Kwiatkowski–Phillips–Schmidt–Shin (KPSS)\n* Philips-Perron (PP)\n","metadata":{}},{"cell_type":"markdown","source":"ADF test is a unit root test.  It determines how strongly a time series is defined by a trend. \n-  Null Hypothesis (H0): Null hypothesis of the test is that the time series can be represented by a unit root that is not stationary.\n- Alternative Hypothesis (H1): Alternative Hypothesis of the test is that the time series is stationary.\n\nInterpretation of p value:\n- above $\\alpha$: Accepts the Null Hypothesis (H0), the data has a unit root and is non-stationary.\n- below $\\alpha$ : Rejects the Null Hypothesis (H0), the data is stationary.\n\n","metadata":{}},{"cell_type":"markdown","source":"ACF / PACF","metadata":{}},{"cell_type":"code","source":"plot_acf(X, lags = 12)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-07-06T09:16:28.918012Z","iopub.execute_input":"2021-07-06T09:16:28.918446Z","iopub.status.idle":"2021-07-06T09:16:29.194673Z","shell.execute_reply.started":"2021-07-06T09:16:28.918405Z","shell.execute_reply":"2021-07-06T09:16:29.193558Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_pacf(X, lags = 12)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-07-06T09:16:42.028513Z","iopub.execute_input":"2021-07-06T09:16:42.029023Z","iopub.status.idle":"2021-07-06T09:16:42.277644Z","shell.execute_reply.started":"2021-07-06T09:16:42.028978Z","shell.execute_reply":"2021-07-06T09:16:42.276712Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We can decompose the series to check the components one by one: which parts are responsible for the non-stationary behavior?","metadata":{}},{"cell_type":"code","source":"decomposition = seasonal_decompose(X, model='additive', period =12)","metadata":{"execution":{"iopub.status.busy":"2021-07-06T09:18:25.879109Z","iopub.execute_input":"2021-07-06T09:18:25.879586Z","iopub.status.idle":"2021-07-06T09:18:25.887092Z","shell.execute_reply.started":"2021-07-06T09:18:25.879546Z","shell.execute_reply":"2021-07-06T09:18:25.886016Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"result = adfuller(X)\nprint('ADF Statistic: %f' % result[0])\nprint('p-value: %f' % result[1])\nprint('Critical Values:')\nfor key, value in result[4].items():\n    print('\\t%s: %.3f' % (key, value))","metadata":{"execution":{"iopub.status.busy":"2021-07-06T09:30:20.148712Z","iopub.execute_input":"2021-07-06T09:30:20.149267Z","iopub.status.idle":"2021-07-06T09:30:20.171546Z","shell.execute_reply.started":"2021-07-06T09:30:20.149229Z","shell.execute_reply":"2021-07-06T09:30:20.170299Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"An important caveat: it is useful to remember that statistical tests do not **accept** a hypothesis - we can only fail to reject it. ","metadata":{}},{"cell_type":"code","source":"# skip the start of the series: adfuller does not handle missing values which appear for values within the first full period\nresult = adfuller(decomposition.trend[10:-10])\nprint('ADF Statistic: %f' % result[0])\nprint('p-value: %f' % result[1])","metadata":{"execution":{"iopub.status.busy":"2021-07-06T09:27:36.858355Z","iopub.execute_input":"2021-07-06T09:27:36.858805Z","iopub.status.idle":"2021-07-06T09:27:36.875831Z","shell.execute_reply.started":"2021-07-06T09:27:36.858752Z","shell.execute_reply":"2021-07-06T09:27:36.874804Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"result = adfuller(decomposition.seasonal[10:-10])\nprint('ADF Statistic: %f' % result[0])\nprint('p-value: %f' % result[1])","metadata":{"execution":{"iopub.status.busy":"2021-07-06T09:27:44.939323Z","iopub.execute_input":"2021-07-06T09:27:44.939963Z","iopub.status.idle":"2021-07-06T09:27:44.953677Z","shell.execute_reply.started":"2021-07-06T09:27:44.939923Z","shell.execute_reply":"2021-07-06T09:27:44.952836Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"result = adfuller(decomposition.resid[10:-10])\nprint('ADF Statistic: %f' % result[0])\nprint('p-value: %f' % result[1])","metadata":{"execution":{"iopub.status.busy":"2021-07-06T09:27:50.917235Z","iopub.execute_input":"2021-07-06T09:27:50.917785Z","iopub.status.idle":"2021-07-06T09:27:50.931154Z","shell.execute_reply.started":"2021-07-06T09:27:50.917735Z","shell.execute_reply":"2021-07-06T09:27:50.930397Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"As we can see, the hypothesis of non-stationarity is non rejected for the trend component, but not for seasonal. The reason for that is that ADF test check for a very specific form of non-stationarity, namely variation in the presence of a linear trend (existence of a single unit root) - while the seasonal component is clearly not stationary (see graph above), it is a qualitatively different kind of behavior.","metadata":{}},{"cell_type":"markdown","source":"Let's go back to our hammer-and-nail approach and try some transformations to make the series stationary.","metadata":{}},{"cell_type":"code","source":"series['passengers2'] = np.log(series['passengers'])\nseries.passengers2.plot()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-07-06T09:33:08.736619Z","iopub.execute_input":"2021-07-06T09:33:08.737106Z","iopub.status.idle":"2021-07-06T09:33:08.979231Z","shell.execute_reply.started":"2021-07-06T09:33:08.737064Z","shell.execute_reply":"2021-07-06T09:33:08.978012Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Applying a logarithm does not remove the trend, but it does seem to stabilize the amplitude (periodic variations have comparable magnitude now). How does that translate into ADF results?","metadata":{}},{"cell_type":"code","source":"result = adfuller(series.passengers2)\nprint('p-value: %f' % result[1])","metadata":{"execution":{"iopub.status.busy":"2021-07-06T09:34:28.485646Z","iopub.execute_input":"2021-07-06T09:34:28.486167Z","iopub.status.idle":"2021-07-06T09:34:28.510137Z","shell.execute_reply.started":"2021-07-06T09:34:28.486117Z","shell.execute_reply":"2021-07-06T09:34:28.508931Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The null is still not rejected, but p-value has dropped - which indicates the transformations are the right way to go. Next, we can try differentiating to get rid of the trend","metadata":{}},{"cell_type":"code","source":"series['passengers3'] = series['passengers'].diff()\nseries.passengers3.plot()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-07-06T09:45:37.648483Z","iopub.execute_input":"2021-07-06T09:45:37.648915Z","iopub.status.idle":"2021-07-06T09:45:37.887031Z","shell.execute_reply.started":"2021-07-06T09:45:37.64888Z","shell.execute_reply":"2021-07-06T09:45:37.886104Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"As expected, differentiation removes the trend (oscillations happen around a fixed level), but variations amplitude is magnified.","metadata":{}},{"cell_type":"code","source":"result = adfuller(series.passengers3[10:])\nprint('ADF Statistic: %f' % result[0])\nprint('p-value: %f' % result[1])","metadata":{"execution":{"iopub.status.busy":"2021-07-06T09:47:14.120307Z","iopub.execute_input":"2021-07-06T09:47:14.120749Z","iopub.status.idle":"2021-07-06T09:47:14.139383Z","shell.execute_reply.started":"2021-07-06T09:47:14.12071Z","shell.execute_reply":"2021-07-06T09:47:14.137943Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We continue moving in the right direction - what happens if we combine the two transformations?","metadata":{}},{"cell_type":"code","source":"series['passengers4'] = series['passengers'].apply(np.log).diff()\nseries.passengers4.plot()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-07-06T09:49:23.716619Z","iopub.execute_input":"2021-07-06T09:49:23.717143Z","iopub.status.idle":"2021-07-06T09:49:23.932589Z","shell.execute_reply.started":"2021-07-06T09:49:23.717104Z","shell.execute_reply":"2021-07-06T09:49:23.931711Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"result = adfuller(series.passengers4[10:])\nprint('ADF Statistic: %f' % result[0])\nprint('p-value: %f' % result[1])","metadata":{"execution":{"iopub.status.busy":"2021-07-06T09:49:37.951813Z","iopub.execute_input":"2021-07-06T09:49:37.952406Z","iopub.status.idle":"2021-07-06T09:49:37.97533Z","shell.execute_reply.started":"2021-07-06T09:49:37.952353Z","shell.execute_reply":"2021-07-06T09:49:37.974364Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"So after applying logarithm (to stabilize the variance) and differentiation (to remove the trend), we have transformed our series to one that can be plausibly treated as stationary. We can verify that intuition by examining ACF and PACF:","metadata":{}},{"cell_type":"code","source":"plot_acf(series['passengers4'][10:], lags = 10)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-07-06T09:52:25.215275Z","iopub.execute_input":"2021-07-06T09:52:25.215636Z","iopub.status.idle":"2021-07-06T09:52:25.430903Z","shell.execute_reply.started":"2021-07-06T09:52:25.215602Z","shell.execute_reply":"2021-07-06T09:52:25.429866Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_pacf(series['passengers4'][10:], lags = 10)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-07-06T09:52:35.265398Z","iopub.execute_input":"2021-07-06T09:52:35.265789Z","iopub.status.idle":"2021-07-06T09:52:35.808136Z","shell.execute_reply.started":"2021-07-06T09:52:35.265737Z","shell.execute_reply":"2021-07-06T09:52:35.806961Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<!-- TODO:\n* lag plots\n* moar explanations? -->","metadata":{}},{"cell_type":"markdown","source":"<!-- <a id=\"section-five\"></a>\n# Causality\n\nMOAR: Granger causality test is used to determine if one time series will be useful to forecast another. It is based on the idea that if X causes Y, then the forecast of Y based on previous values of Y AND the previous values of X should outperform the forecast of Y based on previous values of Y alone.\nSo, Granger causality test should not be used to test if a lag of Y causes Y. Instead, it is generally used on exogenous (not Y lag) variables only. It is implemented in the statsmodel package.\nIt accepts a 2D array with 2 columns as the main argument. The values are in the first column and the predictor (X) is in the second column. The Null hypothesis is that the series in the second column, does not Granger cause the series in the first. If the P-Values are less than a significance level (0.05) then we reject the null hypothesis and conclude that the said lag of X is indeed useful. The second argument maxlag says till how many lags of Y should be included in the test.a -->","metadata":{}}]}